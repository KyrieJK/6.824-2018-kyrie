# 分布式系统基础理论(六)	分布式系统原理

   * [分布式系统基础理论(六)	分布式系统原理](#分布式系统基础理论六分布式系统原理)
      * [数据分布方式](#数据分布方式)
         * [哈希方式](#哈希方式)
         * [按数据范围分布](#按数据范围分布)
         * [按数据量分布](#按数据量分布)
         * [一致性哈希](#一致性哈希)
         * [副本与数据分布](#副本与数据分布)
         * [本地化计算](#本地化计算)

## 数据分布方式

所谓分布式系统就是利用多台计算机协同解决单台计算机所不能解决的计算、存储等问题。单机系统与分布式系统的最大区别在于问题的规模，即计算、存储的数据量的区别。将一个单机问题使用分布式解决，首先要解决如何将单一问题拆分为多机分布式解决，使得分布式系统中的每台机器负责原问题的一个子集。不过无论是解决计算还是存储问题，输入都是数据，所以问题就演变成了如何拆分数据，在分布式系统中通常叫做数据分布问题。下面罗列几种不同的数据分布方式，分别说明一下每个分布方式的不同之处。

### 哈希方式

哈希方式是最常见的数据分布方式，按照数据的某一特征计算哈希值，并将哈希值与机器中的机器建立映射关系，从而将不同哈希值的数据分布到不同的机器上。“数据特征”可以是我们常说的key-value中的key，也可以是其他与应用业务逻辑相关的值。例如，一种常见的哈希方式是按数据属于的用户id计算哈希值，集群中的服务器按0到机器数减1编号，哈希值除以服务器的个数，结果的余数作为处理该数据的服务器编号。但是由于考虑到系统可用性，通常需要多副本做灾备。所以我们需要考虑服务器的副本冗余，将每数台(例如3)服务器组成一组，用哈希值除以总的组数，其余数为服务器组的编号。下图给出了哈希方式分数据的一个例子，将数据按哈希值分配到4个节点上。

![](https://raw.githubusercontent.com/KyrieJK/Figurebed/master/img/20191127235746.png)

我们可以将哈希方式想象成一个大的HashTable，每台机器就是HashTable中的桶，数据根据哈希值被分布到各个桶上面。只要哈希函数的散列性较好，哈希方式可以较为均匀的将数据分布到集群中去。哈希方式需要记录的元信息也非常简单，任何时候，任何节点只需要知道哈希函数的计算方式及模的服务器总数就可以计算出处理具体数据的机器是哪台。

哈希分布数据的缺点也很明显，突出表现为可扩展性较差。一旦集群规模需要扩展，则几乎所有的数据需要被迁移并重新分布。工程中，扩展哈希分布数据的系统时，为了防止所有的数据需要重新分布，往往使集群规模成倍扩展，按照数据重新计算哈希，这样原本一台机器上的数据只需迁移一半到另一台对应的机器上即可完成扩展。

针对哈希方式扩展性的问题，一种思路是不再简单的将哈希值与机器做除法取模映射，而是将对应关系作为元数据由专门的元数据服务器管理。访问数据时，首先计算哈希值并查询元数据服务器，获得该哈希值对应的机器。同时，哈希值取模个数往往大于机器个数，这样同一台机器上需要负责多个哈希取模的余数。在集群扩容时，将部分余数分配到新加入的机器并迁移对应的数据到新机器上，从而使得扩容不再依赖于机器数量的成本增长。这种方式有一个缺点就是需要维护大量的元数据。

哈希分布方式十分依赖哈希函数的散列性。如果哈希函数散列性不好，这就会导致根据某数据特征值的数据严重不均，容易出现“数据倾斜”(data skew)问题。例如，某系统中以用户id做哈希分数据，当某个用户id的数据量异常庞大时，该用户的数据始终由某一台服务器处理，假如该用户的数据量超过了单台服务器处理能力的上限，则该用户的数据无法被处理。更为严重的是，无论如何扩展集群规模，该用户的数据始终只能由某一台服务器处理，都无法解决这个问题。当使用用户id分数据，且用户id=1的数据特别多时，该用户的数据全部堆积到节点2上。

![](https://raw.githubusercontent.com/KyrieJK/Figurebed/master/img/20191128104515.png)

这种情况就是哈希函数的散列性问题，因此需要重新选择哈希函数中选择的数据特征。比如选择用户id与另一个数据维度的组合作为哈希函数的输入，如这样做，则需要完全重新分布数据，在工程实践中可操作性不高。另一种极端的思路是，使用数据的全部而不是某些数据特征计算哈希函数，这样数据将被完全打散在集群中。但是同样的，在工程实践中通常不这样做，因为这会使得每个数据之间的关联性完全消失。

### 按数据范围分布

按数据范围分布是另一个常见的数据分布方式，将数据按特征值的值域范围划分为不同的区间，使得集群中每台服务器处理不同区间的数据。

例如，已知某分布式系统中用户id的值域范围是[1,100)，集群有3台服务器，使用按数据范围划分数据的数据分布方式。将用户id的值域分为三个区间[1,33),[33,90),[90,100)分别由3台服务器负责处理。

![](https://raw.githubusercontent.com/KyrieJK/Figurebed/master/img/20191128113852.png)

每个数据区间的数据大小和区间大小是没有关系的。工程中，为了数据迁移等负载均衡操作的方便，往往利用动态划分区间的技术，使得每个区间中服务的数据量尽量的一样多。当某个区间的数据量较大时，通过将区间分裂的方式拆分为两个区间，使得每个数据区间中的数据量都尽量维持在一个较为固定的阈值之下。

与哈希分布数据的方式只需要记录哈希函数及分桶个数不一样，按数据范围分布数据需要记录所有的数据分布情况。这种方式需要使用专门的服务器在内存中维护数据分布信息，我们称这种数据的分布信息为一种元信息。甚至对于大规模的集群，由于元信息的规模非常庞大，单台计算机无法独立维护，需要使用多台机器作为元信息服务器。

举例来说，分布式系统采用数据范围分布的方式分布数据，每个数据分区中保存256MB的数据，每个数据分区有3个副本。每台服务器有10TB的存储容量，集群规模为1000台服务器。每个数据分区需要1KB的元信息记录数据分区情况及副本所在的服务器。1000台服务器的总存储量为10000TB，总分区数为10000TB/256MB=40M，由于使用3副本，则独立分区数为40M/3=13M，需要的元信息量为13M*1KB=13GB。假设考虑到读写压力，单个元数据服务器可以维护的元数据量为2GB，则需要7台元数据服务器。

实际工程中，一般也不按照某一维度划分数据范围，而是使用全部数据划分范围，从而避免数据倾斜问题。哈希分布数据的方式使得系统中的数据类似一张HashTable。按数据范围划分数据的方式则使得从全局看数据类似一个B树。每个具体的服务器都是B树的叶子节点，元数据服务器是B树的中间节点。

使用按数据范围分布数据的方式的最大优点是可以灵活的根据数据量的具体情况拆分原有数据区间，拆分后的数据区间可以迁移到其他机器，一旦需要集群完成负载均衡时，与哈希方式相比非常灵活。当集群需要扩容时，可以随意添加机器，而不限为倍增的方式，只需将原机器上的部分数据分区迁移到新加入的机器上就可以完成集群扩容。

按数据范围分布数据方式的缺点是需要维护比较复杂的元信息。随着集群规模的增长，元数据服务器比较容易成为瓶颈，从而需要多个元数据服务器机制解决这个问题。

### 按数据量分布

还有一类常用的数据分布方式则是按照数据量分布数据。与哈希方式和按数据范围方式不同，数据量分布数据与具体的数据特征无关，而是将数据视为一个顺序增长的文件，并将这个文件按照某一较为固定的大小划分为若干数据块(chunk)，不同的数据块分布到不同的服务器上。与按数据范围分布数据的方式类似的是，按数据量分布数据也需要记录数据块的具体分布情况，并将该分布信息作为元数据，使用元数据服务器管理。

由于与具体的数据内容无关，按数据量分布数据的方式一般没有数据倾斜问题，数据总是被均匀切分并分布到集群中。当集群需要重新负载均衡时，只需通过迁移数据块即可完成。集群扩容也没有太大的限制，只需要将部分数据块迁移到新加入的机器上即可完成扩容。按数据量划分数据的缺点是需要管理比较复杂的元信息，与按数据范围分布的方式类似，当集群规模较大时，元信息的数据量也变得很大，高效的管理元信息成为一个新的问题。

### 一致性哈希

一致性哈希是另一个在工程中常常使用的数据分布方式。一致性哈希最初在P2P网络中作为分布式哈希表的常用数据分布算法。一致性哈希的基本方式是使用一个哈希函数计算数据或数据特征的哈希值，令该哈希函数的输出值域为一个封闭的环，即哈希函数输出的最大值是最小值的前序。将节点随机分布到这个环上，每个节点负责处理从自己开始顺时针至下一个节点的全部哈希值域上的数据。

例：某一致性哈希函数的值域为[0,10)，系统有三个节点处于的一致性哈希的位置分别为1，4，9，则节点A负责的值域范围为[1,4)，节点B负责的范围为[4,9)，节点C负责的范围为[9,10)和[0,1)。若某数据的哈希值为3，则该数据应由节点A负责处理。

![](https://raw.githubusercontent.com/KyrieJK/Figurebed/master/img/20191128173829.png)

哈希分布数据的方式在集群扩容时非常负责，往往需要倍增节点个数，而一致性哈希的优点在于可以任意动态增加、删除节点，每次添加、删除一个节点仅影响一致性哈希环上相邻的节点。

例：假设需要在哈希环中增加一个新节点D，为D分配的哈希位置为3，则首先将节点A中[3,4)的数据从节点A中拷贝到节点D，然后加入节点D即可。

使用一致性哈希的方式需要将节点在一致性哈希环上的位置作为元信息加以管理，这点比直接使用哈希分布数据的方式要复杂。然而，节点的位置信息只于集群中的机器规模有关，其元信息的量通常比按数据范围分布数据和按数据量分布数据的元信息量要小很多。

上述最基本的一致性哈希算法有很明显的缺点，随机分布节点的方式使得很难均匀的分布哈希值域，尤其在动态增加节点后，即使原先的分布均匀也很难保证继续均匀，由此带来的另一个比较严重的缺点是，当一个节点异常时，该节点的压力全部转移到相邻的一个节点，当加入一个新节点时只能为一个相邻节点分摊压力。

为此一种常见的改进算法是引入虚节点的概念，系统初始时就创建许多虚节点，虚节点的个数一般远大于未来集群中机器的个数，将虚节点均匀分布到一致性哈希值域环上，其功能与基本一致性哈希算法中的节点相同。为每个节点分配若干虚节点。操作数据时，首先通过数据的哈希值在环上找到对应的虚节点，进而查找元数据找到对应的真实节点。使用虚节点改进有多个优点。首先，一旦某个节点不可用，该节点将使得多个虚节点不可用，从而使得多个相邻的真实节点负载失效节点的压力。同理，一旦加入一个新节点，可以分配多个虚节点，从而使得新节点可以负载多个原有节点的压力，从全局看，比较容易实现扩容时的负载均衡。

### 副本与数据分布

分布式系统容错、提高可用性的基本手段就是使用副本。对于数据副本的分布方式主要影响系统的可扩展性。一种基本的数据副本策略是以机器为单位，若干机器互为副本，副本机器之间的数据完全相同。这种策略适用于上述各种数据分布方式。但是这样数据恢复效率并不高、可扩展性也不高。

下图中机器1、2、3互为副本，机器4、5、6互为副本，机器7、8、9互为副本。

![](https://raw.githubusercontent.com/KyrieJK/Figurebed/master/img/20191128192710.png)

这种方式一旦出现副本数据丢失，需要恢复副本数据时效率不高。假设有3个副本机器，突然某台机器磁盘损坏，丢失了全部数据，此时使用新的机器替代故障机器，为了使得新机器也可以提供服务，需要从正常的两台机器上拷贝数据。这种全盘拷贝数据一般都较为消耗资源，为了不影响服务质量，实践中往往采用两种方式：

- 将一台可用的副本机器下线，专门作为数据源拷贝数据，这样做的缺点是造成实际正常副本数只有1个，对数据安全性造成巨大隐患，且如果服务由于分布式系统设计阶段的要求必须有2个副本才能正常提供服务，那么这种做法是完全行不通的。
- 第二，以较低的资源使用率的方法(限速)从两个正常副本上拷贝数据，此方法不停服务，但可以选择服务压力较小的时段进行。该方法的缺点是速度较慢，如果需要恢复的数据量巨大，限速较小，往往需要数天才能完成数据拷贝。

而且，这种方式不利用提高系统扩展性。假设系统原有3台机器，互为副本，现在如果只增加两台机器，由于2台机器无法组成新的副本组，则无法扩容。

另外这种方式不利于系统容错。当出现一台机器宕机时，该机器上的原有压力只能被剩下的副本机器承担，假设有3个副本，宕机一台后，剩下两台的压力将增加50%，有可能直接超过单台的处理能力。理想的情况是，若集群有N台机器，宕机一台后，该台机器的压力可以均匀分散到剩下的N-1台机器上，每台机器的压力仅仅增加1/N-1。

更合适的做法不是以机器作为副本单位，而是将数据拆分为数据段，以数据段为单位作为副本。实践中，常常使得每个数据段的大小尽量相等且控制在一定的大小内。数据段在不同的工程实践以及论文中有很多不同的叫法，segment、fragment、chunk、partition等等。数据段的选择与数据分布方式直接相关，对于按数据量分数据的方式，我们可以自然地按照每个数据块作为数据段。

一旦将数据分为数据段，则可以以数据段为单位管理副本，从而副本与机器不再强相关，每台机器都可以负责一定数据段的副本。

假设分布式系统中的数据有3个数据段o、p、q，每个数据段都有三个副本，系统中有4台机器，第一台机器上有数据段o、p、q；第二台机器上有数据段o、p；第三台机器上有数据段p、q；第四台机器上有数据段q、o。

![](https://raw.githubusercontent.com/KyrieJK/Figurebed/master/img/20191128233713.png)

一旦副本分布与机器不再强相关，数据丢失后的恢复效率将非常高。这是因为，一旦某台机器的数据丢失，其上数据段的副本将分布在整个集群的所有机器中，而不是仅在几个副本及其中，从而可以从整个集群同时拷贝恢复数据，而集群中每台数据源机器都可以以非常低的资源做拷贝。作为恢复数据源的机器即使都限速1MB/s，若有100台机器参与恢复，恢复速度也能达到100MB/s。这种副本分布方式与机器无关也有利于集群容错。如果出现机器宕机，由于宕机机器上的副本分散于整个集群，其压力也自然分散到整个集群。最后，副本分布与机器无关也利于集群扩展。假设集群规模为N台机器，当加入一台新的机器时，只需从各台机器上迁移1/N-1/N+1比例的数据段到新机器即实现了新的负载均衡。由于是从集群中各机器迁移数据，与数据恢复同理，迁移效率也很高。

工程实践中，完全按照数据段建立副本会引起需要管理的元数据的开销增大，副本维护的难度也会增大。一种折中的做法是将某些数据段组成一个数据段分组，按数据段分组为粒度进行副本管理。这样可以将副本粒度控制在一个比较合理的范围内。

### 本地化计算

对于分布式系统而言，除了解决大规模存储问题以外还需要解决大规模的计算问题。然而计算的输入也是数据，计算的规模往往与输入的数据量或者计算产生的中间结果的数据量正相关。在分布式系统中，数据的分布方式也深深影响着计算的分布方式。

在分布式系统中，一种重要的思想就是：移动数据不如移动计算。我们在设计分布式系统时，尽量让计算做到本地化，尽最大可能减少数据移动。如果计算节点和存储节点位于不同的物理机器上，则计算的数据需要通过网络传输，此种方式的开销很大，网络带宽会成为系统的瓶颈。如果将计算尽量调度到与存储节点在同一台物理机器上的计算节点进行，这就称之为本地化计算。